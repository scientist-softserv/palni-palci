version: '3.8'

x-app: &app
  build:
    context: .
    target: hyku-base
    args:
      - EXTRA_APK_PACKAGES=less vim bash openjdk11-jre ffmpeg rsync exiftool
      - HYKU_BULKRAX_ENABLED=true
  image: ghcr.io/scientist-softserv/palni-palci:${TAG:-latest}
  env_file:
    - .env
  # NOTE: all common env variables moved to .env
  volumes:
    - node_modules:/app/samvera/hyrax-webapp/node_modules:cached
    - uploads:/app/samvera/hyrax-webapp/public/uploads:cached
    - assets:/app/samvera/hyrax-webapp/public/assets:cached
    - cache:/app/samvera/hyrax-webapp/tmp/cache:cached
    - .:/app/samvera/hyrax-webapp
  networks:
    internal:

x-app-worker: &app-worker
  <<: *app
  build:
    context: .
    target: hyku-worker
    args:
      - EXTRA_APK_PACKAGES=less vim bash openjdk11-jre ffmpeg rsync exiftool
      - HYKU_BULKRAX_ENABLED=true
    cache_from:
      - ghcr.io/scientist-softserv/palni-palci:${TAG:-latest}
      - ghcr.io/scientist-softserv/palni-palci/worker:${TAG:-latest}
  image: ghcr.io/scientist-softserv/palni-palci/worker:${TAG:-latest}
  command: sh -l -c 'bundle && bundle exec sidekiq'
  depends_on:
    check_volumes:
      condition: service_completed_successfully
    initialize_app:
      condition: service_completed_successfully
    db:
      condition: service_started
    solr:
      condition: service_started
    fcrepo:
      condition: service_started
    redis:
      condition: service_started
    zoo:
      condition: service_started

volumes:
  assets:
  cache:
  db:
  fcrepo:
  node_modules:
  redis:
  solr:
  uploads:
  zk:
  zoo:

networks:
  internal:

services:
  zoo:
    image: zookeeper:3.6.2
    ports:
      - 2181:2181
      - 7001:7000
    environment:
      - ZOO_MY_ID=1
      - ZOO_SERVERS=server.1=zoo:2888:3888;2181
      - ZOO_4LW_COMMANDS_WHITELIST=mntr,srvr,ruok,conf
    volumes:
      - zoo:/data
      - zk:/datalog
    networks:
      internal:
    healthcheck:
      test: ["CMD-SHELL", "echo 'ruok' | nc -w 2 -q 2 localhost 2181 | grep imok || exit 1"]
      interval: "10s"
      timeout: "8s"

  solr:
    image: ghcr.io/samvera/hyku/solr:${SOLR_TAG:-latest}
    build:
      context: solr
      dockerfile: Dockerfile
    environment:
      - OOM=script
      - SOLR_ADMIN_USER=solr
      - SOLR_ADMIN_PASSWORD=SolrRocks
      - SOLR_COLLECTION=hydra-development
      - SOLR_CLOUD_BOOTSTRAP=yes
      - SOLR_ENABLE_CLOUD_MODE=yes
      - SOLR_ENABLE_AUTHENTICATION=yes
      - ZK_HOST=zoo:2181
      - VIRTUAL_PORT=8983
      - VIRTUAL_HOST=solr.hyku.test
    user: root
    command: bash -c "
      chown -R 8983:8983 /var/solr
      && ./bin/solr zk cp file:/var/solr/data/security.json zk:/security.json
      && runuser -u solr -- solr-foreground"
    expose:
      - 8983
    volumes:
      - solr:/var/solr
    networks:
      internal:
    healthcheck:
      test: curl -sf http://$$SOLR_ADMIN_USER:$$SOLR_ADMIN_PASSWORD@solr:8983/solr/admin/cores?action=STATUS || exit 1
      start_period: 3s
      interval: 5s
      timeout: 5s
      retries: 6
    depends_on:
      zoo:
        condition: service_healthy

  fcrepo:
    image: ghcr.io/samvera/fcrepo4:4.7.5
    volumes:
      - fcrepo:/data:cached
    env_file:
      - .env
    environment:
      - VIRTUAL_PORT=8080
      - VIRTUAL_HOST=fcrepo.hyku.test
      - JAVA_OPTS=${JAVA_OPTS} -Dfcrepo.modeshape.configuration="classpath:/config/file-simple/repository.json" -Dfcrepo.object.directory="/data/objects" -Dfcrepo.binary.directory="/data/binaries"
    expose:
      - 8080
    networks:
      internal:

  db:
    image: postgres:11.1
    env_file:
      - .env
    environment:
      - POSTGRES_DB=${DB_NAME}
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_USER=${DB_USER}
      - VIRTUAL_PORT=5432
      - VIRTUAL_HOST=db.hyku.test
    volumes:
      - db:/var/lib/postgresql/data
    networks:
      internal:

  web:
    <<: *app
    # command: sh -l -c "bundle && bundle exec puma -v -b tcp://0.0.0.0:3000"
    environment:
      - VIRTUAL_PORT=3000
      - VIRTUAL_HOST=.hyku.test
    depends_on:
      db:
        condition: service_started
      solr:
        condition: service_started
      fcrepo:
        condition: service_started
      redis:
        condition: service_started
      zoo:
        condition: service_started
      check_volumes:
        condition: service_started
      chrome:
        condition: service_started
      worker:
        condition: service_started
      worker_resource_intensive:
        condition: service_started
      initialize_app:
        condition: service_completed_successfully
    # ports:
    #   - 3000:3000
    expose:
      - 3000

  worker:
    <<: *app-worker

  worker_resource_intensive:
    <<: *app-worker
    command: sh -l -c 'bundle && bundle exec sidekiq -C config/sidekiq_resource_intensive.yml'

  # Do not recurse through all of tmp. derivitives will make booting
  # very slow and eventually just time out as data grows
  check_volumes:
    <<: *app
    user: root
    entrypoint: ["sh", "-x", "-c"]
    command:
      - >
        chown -R app:app /app/samvera/hyrax-webapp/public/uploads &&
        chown -R app:app /app/samvera/hyrax-webapp/public/assets &&
        chown -R app:app /app/samvera/hyrax-webapp/tmp/cache

  initialize_app:
    <<: *app
    environment:
      - CONFDIR=/app/samvera/hyrax-webapp/solr/conf
    entrypoint: ["sh", "-c"]
    command:
      - >
        solrcloud-upload-configset.sh /app/samvera/hyrax-webapp/solr/conf &&
        solrcloud-assign-configset.sh &&
        SOLR_COLLECTION_NAME=hydra-test solrcloud-assign-configset.sh &&
        db-migrate-seed.sh
    depends_on:
      db:
        condition: service_started
      solr:
        condition: service_healthy
      fcrepo:
        condition: service_started
      redis:
        condition: service_started

  redis:
    image: redis:5
    command: redis-server
    volumes:
      - redis:/data
    networks:
      internal:

  chrome:
    image: seleniarm/node-chromium:4.0.0-beta-1-20210215
    volumes:
      - /dev/shm:/dev/shm
    depends_on:
      - seleniarm-hub
    environment:
      - SE_EVENT_BUS_HOST=seleniarm-hub
      - SE_EVENT_BUS_PUBLISH_PORT=4442
      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443
    ports:
      - "6900:5900"
    networks:
      internal:

  seleniarm-hub:
    image: seleniarm/hub:4.0.0-beta-1-20210215
    container_name: seleniarm-hub-4.0.0-beta-1-20210215

    ports:
      - "4442:4442"
      - "4443:4443"
      - "4444:4444"
    networks:
      internal:
